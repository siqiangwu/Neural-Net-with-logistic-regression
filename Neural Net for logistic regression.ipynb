{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Project1_NN_empty.ipynb","provenance":[{"file_id":"1bsY_oxVjvrRmCYQl_64x_7XatcEjnsCh","timestamp":1571339390584},{"file_id":"1Y0bfNpPxW92sJxYZaSwpZDTTO1414tw-","timestamp":1571339345461}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Y1LJdNtE3Fqn","colab_type":"text"},"source":["## Load packages"]},{"cell_type":"code","metadata":{"id":"6Gqh8kPlgiFJ","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from keras.datasets import mnist"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5hWACQotBtYJ","colab_type":"text"},"source":["## Define functions"]},{"cell_type":"markdown","metadata":{"id":"j0Lx7rAg3L4o","colab_type":"text"},"source":["### Sigmoid"]},{"cell_type":"code","metadata":{"id":"bm7_VffO3CFb","colab_type":"code","colab":{}},"source":["def sigmoid(z):\n","    \"\"\"\n","    Compute the sigmoid of z\n","\n","    Arguments:\n","    x -- A scalar or numpy array of any size.\n","\n","    Return:\n","    s -- sigmoid(z)\n","    \"\"\"\n","\n","   ?? Add here ??\n","    \n","    return s"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S9Jv5CfG3LQk","colab_type":"text"},"source":["### Initialize weights"]},{"cell_type":"code","metadata":{"id":"StsRU276CPv4","colab_type":"code","colab":{}},"source":["def initialize_weights(dim):\n","    \"\"\"\n","    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n","    \n","    Argument:\n","    dim -- size of the w vector we want (or number of parameters in this case)\n","    \n","    Returns:\n","    w -- initialized vector of shape (dim, 1)\n","    b -- initialized scalar (corresponds to the bias)\n","    \"\"\"\n","\n"," ?? Add here ??\n","  \n","    assert(w.shape == (dim, 1))\n","    assert(isinstance(b, float) or isinstance(b, int))\n","    \n","    return w, b"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mY_hYmALCSW2","colab_type":"text"},"source":["### Forward and backward propagation"]},{"cell_type":"code","metadata":{"id":"_YDXRCMoCW18","colab_type":"code","colab":{}},"source":["def propagate(w, b, X, Y):\n","    \"\"\"\n","    Implement the cost function and its gradient for the propagation explained in the assignment\n","\n","    Arguments:\n","    w -- weights, a numpy array of size (num_px * num_px, 1)\n","    b -- bias, a scalar\n","    X -- data of size (number of examples, num_px * num_px)\n","    Y -- true \"label\" vector of size (1, number of examples)\n","\n","    Return:\n","    cost -- negative log-likelihood cost for logistic regression\n","    dw -- gradient of the loss with respect to w, thus same shape as w\n","    db -- gradient of the loss with respect to b, thus same shape as b\n","    \n","    \"\"\"\n","    \n","    m = X.shape[0]\n","    \n","    # FORWARD PROPAGATION (FROM X TO COST)\n","   \n","    ?? Add here ??  \n","    \n","    # BACKWARD PROPAGATION (TO FIND GRAD)\n","   \n","    ?? Add here ??\n","\n","    assert(dw.shape == w.shape)\n","    assert(db.dtype == float)\n","    cost = np.squeeze(cost)\n","    assert(cost.shape == ())\n","    \n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","    \n","    return grads, cost"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bAalkyubClnB","colab_type":"text"},"source":["### Gradient descent"]},{"cell_type":"code","metadata":{"id":"_oWQ26xrrPRZ","colab_type":"code","colab":{}},"source":["def gradient_descent(w, b, X, Y, num_iterations, learning_rate):\n","    \"\"\"\n","    This function optimizes w and b by running a gradient descent algorithm\n","    \n","    Arguments:\n","    w -- weights, a numpy array of size (num_px * num_px, 1)\n","    b -- bias, a scalar\n","    X -- data of shape (num_px * num_px, number of examples)\n","    Y -- true \"label\" vector of shape (1, number of examples)\n","    num_iterations -- number of iterations of the optimization loop\n","    learning_rate -- learning rate of the gradient descent update rule\n","    \n","    Returns:\n","    params -- dictionary containing the weights w and bias b\n","    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n","    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n","    \n","    Tips:\n","    You basically need to write down two steps and iterate through them:\n","        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n","        2) Update the parameters using gradient descent rule for w and b.\n","    \"\"\"\n","    \n","    costs = []\n","    \n","    for i in range(num_iterations):\n","        \n","        \n","        # Cost and gradient calculation\n","      \n","        grads, cost = propagate(w, b, X, Y)\n","      \n","        # Retrieve derivatives from grads\n","        dw = grads[\"dw\"]\n","        db = grads[\"db\"]\n","        \n","        # update rule\n","        \n","    ?? Add here ??  \n","        # Record the costs\n","        if i % 100 == 0:\n","            costs.append(cost)\n","            # Print the cost every 100 training examples\n","            print (\"Cost after iteration %i: %f\" % (i, cost))\n","    \n","    params = {\"w\": w,\n","              \"b\": b}\n","    \n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","    \n","    return params, grads, costs\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xp3Zv_BTCrik","colab_type":"text"},"source":["### Make predictions"]},{"cell_type":"code","metadata":{"id":"rU5EFchuCtX_","colab_type":"code","colab":{}},"source":["def predict(w, b, X):\n","    '''\n","    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n","    \n","    Arguments:\n","    w -- weights, a numpy array of size (num_px * num_px, 1)\n","    b -- bias, a scalar\n","    X -- data of size (num_px * num_px, number of examples)\n","    \n","    Returns:\n","    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n","    '''\n","    \n","    m = X.shape[0]\n","    Y_prediction = np.zeros((1, m))\n","    w = w.reshape(X.shape[1], 1)\n","    \n","    # Compute vector \"A\" predicting the probabilities of the picture containing a 1\n","    \n","    A = sigmoid(np.dot(w.T, X.T) + b)\n","    \n","    \n","    for i in range(A.shape[1]):\n","        # Convert probabilities A[0,i] to actual predictions p[0,i]\n","        \n","    ?? Add here ??  \n","\n","    \n","    assert(Y_prediction.shape == (1, m))\n","    \n","    return Y_prediction"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OaWaPhn0C37b","colab_type":"text"},"source":["## Merge functions and run your model"]},{"cell_type":"code","metadata":{"id":"YwuPhKu9gr3C","colab_type":"code","colab":{}},"source":["# LOAD DATA\n","class0 = 0\n","class1 = 1\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train = x_train[np.isin(y_train,[class0,class1]),:,:]\n","y_train = 1*(y_train[np.isin(y_train,[class0,class1])]>class0)\n","x_test = x_test[np.isin(y_test,[class0,class1]),:,:]\n","y_test = 1*(y_test[np.isin(y_test,[class0,class1])]>class0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0EpvnpT4hok1","colab_type":"code","colab":{}},"source":["# RESHAPE\n","\n","x_train_flat = x_train.reshape(x_train.shape[0],-1)\n","print(x_train_flat.shape)\n","print('Train: '+str(x_train_flat.shape[0])+' images and '+str(x_train_flat.shape[1])+' neurons \\n')\n","\n","x_test_flat = x_test.reshape(x_test.shape[0],-1)\n","print(x_test_flat.shape)\n","print('Test: '+str(x_test_flat.shape[0])+' images and '+str(x_test_flat.shape[1])+' neurons \\n')\n","\n","# STRANDARIZE\n","x_train_flat = x_train_flat / 255\n","x_test_flat = x_test_flat / 255"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UU_yVAHXIpbf","colab_type":"text"},"source":["### Train the model (in training set)"]},{"cell_type":"code","metadata":{"id":"Dfbhuv6Li00c","colab_type":"code","colab":{}},"source":["# Initialize parameters with zeros (≈ 1 line of code)\n","w, b = initialize_weights(x_train_flat.shape[1])\n","\n","# Gradient descent (≈ 1 line of code)\n","learning_rate = 0.005\n","num_iterations = 2000\n","parameters, grads, costs = gradient_descent(w, b, x_train_flat, y_train, 2000, 0.005)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qavG0L1TIu5d","colab_type":"text"},"source":["### Test the model (in testing set)"]},{"cell_type":"code","metadata":{"id":"zitDghtdIvD1","colab_type":"code","colab":{}},"source":["# Retrieve parameters w and b from dictionary \"parameters\"\n","w = parameters[\"w\"]\n","b = parameters[\"b\"]\n","    \n","# Predict test/train set examples (≈ 2 lines of code)\n","y_prediction_test = predict(w, b, x_test_flat)\n","y_prediction_train = predict(w, b, x_train_flat)\n","\n","# Print train/test Errors\n","print('')\n","print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n","print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n","print('')\n","\n","plt.figure(figsize=(13,5))\n","plt.plot(range(0,2000,100),costs)\n","plt.title('Cost training vs iteration')\n","plt.xlabel('Iterations')\n","plt.ylabel('Cost')\n","plt.xticks(range(0,2000,100))\n","\n","\n","plt.figure(figsize=(13,5))\n","plt.imshow(w.reshape(28,28))\n","plt.title('Template')\n"],"execution_count":0,"outputs":[]}]}